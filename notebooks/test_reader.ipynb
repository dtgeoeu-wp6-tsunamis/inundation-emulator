{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_DIR = \"/home/ebr/projects/inundation-emulator\"\n",
    "os.chdir(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ebr/data/PTHA2020_runs_UMA/train_164'\n",
    "TOPOFILE = '/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd'\n",
    "TOPO_MASK = '/home/ebr/data/PTHA2020_runs_UMA/Catania/ct_mask.txt'\n",
    "#GENERATED_DIR = os.path.join(ROOT_DIR, 'generated')\n",
    "#SUMMARIES_TRAIN = os.path.join(DATA_DIR, 'train_test/train.txt')\n",
    "#SUMMARIES_VAL = os.path.join(DATA_DIR, 'train_test/validate.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def inspect_nc_file(file_path):\n",
    "    # Open the NetCDF file\n",
    "    dataset = Dataset(file_path, 'r')\n",
    "    \n",
    "    # Print general information about the dataset\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Dimensions:\")\n",
    "    pprint(dataset.dimensions)\n",
    "    \n",
    "    print(f\"\\nVariables:\")\n",
    "    pprint(dataset.variables)\n",
    "    \n",
    "    print(f\"\\nGlobal Attributes:\")\n",
    "    pprint(dataset.ncattrs())\n",
    "    for attr in dataset.ncattrs():\n",
    "        print(f\"{attr}: {getattr(dataset, attr)}\")\n",
    "    \n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "# Example usage\n",
    "file_path = '/home/ebr/data/PTHA2020_runs_UMA/train_164/1357_E02020N3739E02658N3366-PS-Mur_PYes_Var-M895_E02426N3465_S002_CT_10m.nc'\n",
    "#file_path = \"/home/ebr/data/PTHA2020_runs_UMA/Catania/A_CT.grd\"\n",
    "inspect_nc_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generator to create dataset.\n",
    "\"\"\"\n",
    "Scenario = namedtuple('Scenario', ['eta',\n",
    "                                   'flow_depth'])\n",
    "\n",
    "\n",
    "class DataReader:\n",
    "\n",
    "    def __init__(self, \n",
    "                 scenarios_file, \n",
    "                 pois, \n",
    "                 datadir, \n",
    "                 topofile,\n",
    "                 topo_mask_file=None, \n",
    "                 shuffle_on_load=False, \n",
    "                 reload=False):\n",
    "        \n",
    "        self.scenarios_file = scenarios_file\n",
    "        self.pois = pois\n",
    "        self.datadir = datadir\n",
    "        self.topofile = topofile\n",
    "        self.topo_mask_file = topo_mask_file\n",
    "        self.shuffle_on_load = shuffle_on_load\n",
    "        self.reload = reload\n",
    "        self.lines = None\n",
    "\n",
    "        with Dataset(self.topofile, 'r') as ds:\n",
    "            self.topography = ds.variables[\"z\"][:,:]\n",
    "        \n",
    "        if self.topo_mask_file:        \n",
    "            with open(self.topo_mask_file, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # Convert each line to a boolean (True for \"true\", False for \"false\")\n",
    "            boolean_array = np.array([\n",
    "                [element.strip().lower() == 'true' for element in line.split()]\n",
    "                for line in lines\n",
    "            ], dtype=bool)\n",
    "            \n",
    "            self.topo_mask = boolean_array.T\n",
    "        \n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.scenarios_file, 'r') as file:\n",
    "            self.lines = file.readlines()\n",
    "            if self.shuffle_on_load:\n",
    "                random.shuffle(self.lines)\n",
    "\n",
    "    def generator(self):\n",
    "        self.load()\n",
    "        while self.lines:\n",
    "            line = self.lines.pop()\n",
    "            \n",
    "            # Reload if no more lines and reload is true\n",
    "            if not self.lines and self.reload:\n",
    "                self.load()\n",
    "            \n",
    "            flow_depth, eta, deformed_topography = self.get_sample(line.strip())\n",
    "            scenario = Scenario(eta=eta, flow_depth=flow_depth)\n",
    "            yield scenario\n",
    "\n",
    "    def get_sample(self, scenario):\n",
    "        filename_CT = os.path.join(self.datadir, f\"{scenario}_CT_10m.nc\")\n",
    "        filename_ts = os.path.join(self.datadir, f\"{scenario}_ts.nc\")\n",
    "        \n",
    "        # Initialize the eta array with the proper shape and type\n",
    "        #eta = np.empty(self.in_dims, dtype=np.float32)\n",
    "        with Dataset(filename_ts) as ds:\n",
    "            eta = ds.variables[\"eta\"][:, self.pois]\n",
    "\n",
    "        # Initialize flow_depth and deformed_topography\n",
    "        flow_depth = np.zeros(self.topography.shape)\n",
    "        \n",
    "        with Dataset(filename_CT) as ds:\n",
    "            max_height = ds.variables[\"max_height\"][:,:]\n",
    "            deformation = ds.variables[\"deformation\"][:,:]\n",
    "            deformed_topography = self.topography - deformation\n",
    "\n",
    "            # Create a mask and calculate flow_depth\n",
    "            mask = (self.topography > 0) & (max_height != np.ma.masked) & (max_height > deformed_topography)\n",
    "            flow_depth[mask] = (max_height - deformed_topography)[mask]\n",
    "            \n",
    "            if self.topo_mask_file:\n",
    "                flow_depth = flow_depth[self.topo_mask]\n",
    "                \n",
    "        return flow_depth, eta.T, deformed_topography\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = range(30,45)\n",
    "n_pois = len(pois)\n",
    "\n",
    "reader = DataReader(\n",
    "    scenarios_file=\"/home/ebr/projects/inundation-emulator/training_set/scenarios.txt\",\n",
    "    pois=pois,\n",
    "    datadir=DATA_DIR,\n",
    "    topofile=TOPOFILE,\n",
    "    topo_mask_file=TOPO_MASK,\n",
    "    shuffle_on_load=False, \n",
    "    reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=reader.generator,\n",
    "        output_signature=(\n",
    "                tf.TensorSpec(shape=(n_pois,481), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(reader.topo_mask.sum()), dtype=tf.int32)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,element in enumerate(dataset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the dataset\n",
    "batch_size = 30\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Iterate over the batched dataset and print the results\n",
    "for i, batch in enumerate(batched_dataset):\n",
    "    eta, flow_depth = batch\n",
    "    print(i, flow_depth.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, Input, regularizers\n",
    "reg = 1e-5\n",
    "\n",
    "def get_model():\n",
    "    # Encoder\n",
    "    encoder = models.Sequential([\n",
    "        Input(shape=(n_pois,481,1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', strides=(1, 1), use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 2)),\n",
    "        layers.Conv2D(64, (3, 5), activation='relu', strides=(1, 1), use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 5), strides=(2, 3)),\n",
    "        layers.Conv2D(128, (3, 5), activation='relu', strides=(1, 1), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 5), strides=(2, 3)),\n",
    "        layers.Conv2D(32, (1, 1), activation='relu', strides=(1, 1), use_bias=True, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(16, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg))\n",
    "    ])\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = models.Sequential([\n",
    "        layers.Dense(32, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.Dense(418908, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg))\n",
    "    ])\n",
    "    \n",
    "    # Complete Model\n",
    "    model = models.Sequential([encoder, decoder])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate and build the model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(eta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10  # Adjust to your requirements\n",
    "dataset_size = 164\n",
    "epochs = 10\n",
    "steps_per_epoch = int(dataset_size/batch_size)\n",
    "\n",
    "# Create dataset from generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=reader.generator,\n",
    "        output_signature=(\n",
    "                tf.TensorSpec(shape=(n_pois, 481), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(reader.topo_mask.sum()), dtype=tf.int32)\n",
    "        )\n",
    ").cache().shuffle(buffer_size=dataset_size)\n",
    "batched_dataset = dataset.batch(batch_size)#.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define the model (already done in previous steps)\n",
    "model = get_model()\n",
    "\n",
    "# Compile the model with a loss function and optimizer\n",
    "model.compile(optimizer='adam', \n",
    "              loss=\"mse\",  # Adjust as per your task (e.g., binary crossentropy or MSE)\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(batched_dataset, epochs=epochs)#, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard generated/emulator_20250117_094513/logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inundation-emulator-e-rOiPvR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
