{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_DIR = \"/home/ebr/projects/inundation-emulator\"\n",
    "os.chdir(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ebr/data/PTHA2020_runs_UMA/train_164'\n",
    "TOPOFILE = '/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd'\n",
    "TOPO_MASK = '/home/ebr/data/PTHA2020_runs_UMA/Catania/ct_mask.npy'\n",
    "SCENARIOS = \"/home/ebr/data/PTHA2020_runs_UMA/train_164/scenarios.txt\"\n",
    "GRID_INFO_FILE = '/home/ebr/data/PTHA2020_runs_UMA/Catania/grid_info.json'\n",
    "#GENERATED_DIR = os.path.join(ROOT_DIR, 'generated')\n",
    "#SUMMARIES_TRAIN = os.path.join(DATA_DIR, 'train_test/train.txt')\n",
    "#SUMMARIES_VAL = os.path.join(DATA_DIR, 'train_test/validate.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_reader import DataReader\n",
    "\n",
    "pois = range(30,45)\n",
    "n_pois = len(pois)\n",
    "\n",
    "reader = DataReader(\n",
    "    scenarios_file=SCENARIOS,\n",
    "    pois=pois,\n",
    "    datadir=DATA_DIR,\n",
    "    topofile=TOPOFILE,\n",
    "    topomask_file=TOPO_MASK,\n",
    "    shuffle_on_load=False,\n",
    "    reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.store_grid_info(GRID_INFO_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ebr/data/PTHA2020_runs_UMA/Catania/grid_info.json') as f:\n",
    "    grid_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_info['dimensions']['grid_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)[range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=reader.generator,\n",
    "        output_signature=(\n",
    "                tf.TensorSpec(shape=(n_pois,481), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(reader.topo_mask.sum()), dtype=tf.int32)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,element in enumerate(dataset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the dataset\n",
    "batch_size = 30\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Iterate over the batched dataset and print the results\n",
    "for i, batch in enumerate(batched_dataset):\n",
    "    eta, flow_depth = batch\n",
    "    print(i, flow_depth.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect netcdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def inspect_nc_file(file_path):\n",
    "    # Open the NetCDF file\n",
    "    dataset = Dataset(file_path, 'r')\n",
    "    \n",
    "    # Print general information about the dataset\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Dimensions:\")\n",
    "    pprint(dataset.dimensions)\n",
    "    \n",
    "    print(f\"\\nVariables:\")\n",
    "    pprint(dataset.variables)\n",
    "    \n",
    "    print(f\"\\nGlobal Attributes:\")\n",
    "    pprint(dataset.ncattrs())\n",
    "    for attr in dataset.ncattrs():\n",
    "        print(f\"{attr}: {getattr(dataset, attr)}\")\n",
    "    \n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "\n",
    "# Example usage\n",
    "file_path = '/home/ebr/data/PTHA2020_runs_UMA/train_164/1357_E02020N3739E02658N3366-PS-Mur_PYes_Var-M895_E02426N3465_S002_CT_10m.nc'\n",
    "#file_path = '/home/ebr/data/PTHA2020_runs_UMA/train_164/1357_E02020N3739E02658N3366-PS-Mur_PYes_Var-M895_E02426N3465_S002_ts.nc'\n",
    "#file_path = \"/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd\"\n",
    "inspect_nc_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(file_path, 'r')\n",
    "ds_topo = Dataset(\"/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height = ds.variables[\"max_height\"]\n",
    "max_height.set_auto_maskandscale(True)\n",
    "deformation = ds.variables[\"deformation\"]\n",
    "deformation.set_auto_maskandscale(True)\n",
    "topography = ds_topo.variables[\"z\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topography[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deformed_topography = topography[:,:] - deformation[:,:].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(deformed_topography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mask and calculate flow_depth\n",
    "mask = np.logical_and(topography[:,:] > 0, ~max_height[:,:].mask, max_height[:,:] > deformed_topography)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (self.topography > 0) & (max_height > deformed_topography)\n",
    "flow_depth[mask] = (max_height - deformed_topography)[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(max_height[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height[:,:].data > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(max_height[:,:].mask, max_height[:,:].data > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, Input, regularizers\n",
    "reg = 1e-5\n",
    "\n",
    "def get_model():\n",
    "    # Encoder\n",
    "    encoder = models.Sequential([\n",
    "        Input(shape=(n_pois,481,1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', strides=(1, 1), use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 2)),\n",
    "        layers.Conv2D(64, (3, 5), activation='relu', strides=(1, 1), use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 5), strides=(2, 3)),\n",
    "        layers.Conv2D(128, (3, 5), activation='relu', strides=(1, 1), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.MaxPooling2D(pool_size=(3, 5), strides=(2, 3)),\n",
    "        layers.Conv2D(32, (1, 1), activation='relu', strides=(1, 1), use_bias=True, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(16, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg))\n",
    "    ])\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = models.Sequential([\n",
    "        layers.Dense(32, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg)),\n",
    "        layers.Dense(418908, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(reg))\n",
    "    ])\n",
    "    \n",
    "    # Complete Model\n",
    "    model = models.Sequential([encoder, decoder])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate and build the model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(eta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10  # Adjust to your requirements\n",
    "dataset_size = 164\n",
    "epochs = 10\n",
    "steps_per_epoch = int(dataset_size/batch_size)\n",
    "\n",
    "# Create dataset from generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=reader.generator,\n",
    "        output_signature=(\n",
    "                tf.TensorSpec(shape=(n_pois, 481), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(reader.topo_mask.sum()), dtype=tf.int32)\n",
    "        )\n",
    ").cache().shuffle(buffer_size=dataset_size)\n",
    "batched_dataset = dataset.batch(batch_size)#.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define the model (already done in previous steps)\n",
    "model = get_model()\n",
    "\n",
    "# Compile the model with a loss function and optimizer\n",
    "model.compile(optimizer='adam', \n",
    "              loss=\"mse\",  # Adjust as per your task (e.g., binary crossentropy or MSE)\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(batched_dataset, epochs=epochs)#, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard generated/emulator_20250117_094513/logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_reader import DataReader\n",
    "\n",
    "\n",
    "TOPO_FILE = '/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd'\n",
    "TOPO_MASK = '/home/ebr/data/PTHA2020_runs_UMA/Catania/ct_mask.txt'\n",
    "TRAIN_SCENARIOS =\"/home/ebr/data/PTHA2020_runs_UMA/train_591/scenarios.txt\"\n",
    "TRAIN_DIR = '/home/ebr/data/PTHA2020_runs_UMA/train_591'\n",
    "\n",
    "pois = range(30,45)\n",
    "n_pois = len(pois)\n",
    "\n",
    "reader = DataReader(\n",
    "    scenarios_file=TRAIN_SCENARIOS,\n",
    "    pois=pois,\n",
    "    datadir=DATA_DIR,\n",
    "    topofile=TOPOFILE,\n",
    "    topo_mask_file=TOPO_MASK,\n",
    "    shuffle_on_load=False, \n",
    "    reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eta, flow_depth, scenario in reader.generator():\n",
    "    print(flow_depth)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eta[1:15,1:100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inundation-emulator-e-rOiPvR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
