{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_DIR = \"/home/ebr/projects/inundation-emulator\"\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.emulator import Emulator, DataReader\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained emulator.\n",
    "\n",
    "GENERATED_DIR = \"/home/ebr/projects/inundation-emulator/generated\"\n",
    "TOPO_FILE = '/home/ebr/data/PTHA2020_runs_UMA/Catania/C_CT.grd'\n",
    "TOPO_MASK = '/home/ebr/data/PTHA2020_runs_UMA/Catania/ct_mask.txt'\n",
    "TRAIN_SCENARIOS =\"/home/ebr/data/PTHA2020_runs_UMA/train_591/scenarios.txt\"\n",
    "TRAIN_DIR = '/home/ebr/data/PTHA2020_runs_UMA/train_591'\n",
    "VALIDATION_SCENARIOS = '/home/ebr/data/PTHA2020_runs_UMA/test/scenarios.txt'\n",
    "VALIDATION_DIR = \"/home/ebr/data/PTHA2020_runs_UMA/test\"\n",
    "\n",
    "RUNDIR = \"/home/ebr/projects/inundation-emulator/generated/emulator_20250123_220604\"\n",
    "EPOCH_CHECKPOINT = 500\n",
    "\n",
    "emulator = Emulator(GENERATED_DIR, TOPO_FILE, TOPO_MASK, rundir=RUNDIR, epoch_checkpoint=EPOCH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "topomask_file = \"/home/ebr/projects/inundation-emulator/article_data/ct_mask.txt\"\n",
    "\n",
    "\n",
    "with open(topomask_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Convert each line to a boolean (True for \"true\", False for \"false\")\n",
    "topomask = np.array([\n",
    "    [element.strip().lower() == 'true' for element in line.split()]\n",
    "    for line in lines\n",
    "], dtype=bool).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topomask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "scenarios_file = TRAIN_SCENARIOS\n",
    "input_dir = TRAIN_DIR\n",
    "predictions_dir = \"/home/ebr/projects/inundation-emulator/predictions\"\n",
    "batch_size = 10\n",
    "\n",
    "# Create output folder\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = os.path.join(predictions_dir, emulator.id, f\"preds_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(scenarios_file, 'r') as file:\n",
    "    nr_of_scenarios = sum(1 for line in file if line.strip())\n",
    "    print(f\"Number of scenarios for prediction: {nr_of_scenarios}\")\n",
    "\n",
    "reader = DataReader( \n",
    "        scenarios_file=scenarios_file,\n",
    "        pois=emulator.pois,\n",
    "        datadir=input_dir,\n",
    "        topofile=emulator.topofile,\n",
    "        topomask_file=emulator.topomask_file,\n",
    "        shuffle_on_load=False,\n",
    "        target=True,\n",
    "        reload=False\n",
    ")\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(emulator.n_pois, emulator.input_time_steps), dtype=tf.float32),           # eta\n",
    "    tf.TensorSpec(shape=(reader.topomask.sum()), dtype=tf.float32),         # No flow_depth\n",
    "    tf.TensorSpec(shape=(), dtype=tf.string)                               # scenario\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=reader.generator,\n",
    "        output_signature=output_signature\n",
    ").batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((batch_size, *reader.topomask.shape))\n",
    "flow_depths = np.zeros((batch_size, *reader.topomask.shape))\n",
    "\n",
    "for eta, flow_depth, scenario_id in dataset:\n",
    "    # Make predictions\n",
    "    #preds[:, topomask] = emulator.model(eta, training=False)\n",
    "    preds[:, topomask] = emulator.model.predict(eta)\n",
    "    flow_depths[:,topomask] = flow_depth\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator.model.predict(eta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flip(flow_depths[1,:], axis=0))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flip(preds[1,:], axis=0))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from shutil import copyfile\n",
    "\n",
    "for i in range(10):\n",
    "    # Create a NetCDF file for each scenario\n",
    "    scenario_id_str = scenario_id.numpy().decode('utf-8')  # Decode scenario_id to string\n",
    "    pred_file = os.path.join(output_dir, f\"{scenario_id_str}_CT_10m_PR.nc\")\n",
    "    \n",
    "    with Dataset(pred_file, mode='w', format=\"NETCDF4\") as ds:from netCDF4 import Dataset\n",
    "\n",
    "# Input and Output file paths\n",
    "input_file = \"input.nc\"\n",
    "output_file = \"output.nc\"\n",
    "\n",
    "# Open the input file in read mode\n",
    "with Dataset(input_file, \"r\") as src:\n",
    "\n",
    "    # Create the output file\n",
    "    with Dataset(output_file, \"w\", format=\"NETCDF4\") as dst:\n",
    "\n",
    "        # Copy dimensions from the input file\n",
    "        for dim_name, dim in src.dimensions.items():\n",
    "            dst.createDimension(dim_name, (len(dim) if not dim.isunlimited() else None))\n",
    "\n",
    "        # Copy variables related to spatial grid\n",
    "        grid_vars = [\"grid_lat\", \"grid_lon\", \"lat\", \"lon\"]  # Adjust this based on your input file\n",
    "        for var_name in grid_vars:\n",
    "            if var_name in src.variables:\n",
    "                src_var = src.variables[var_name]\n",
    "                \n",
    "                # Create variable in output file with same properties\n",
    "                dst_var = dst.createVariable(var_name, src_var.datatype, src_var.dimensions)\n",
    "                \n",
    "                # Copy attributes\n",
    "                dst_var.setncatts({attr: src_var.getncattr(attr) for attr in src_var.ncattrs()})\n",
    "                \n",
    "                # Copy data\n",
    "                dst_var[:] = src_var[:]\n",
    "\n",
    "        print(\"Successfully copied dimensions and spatial grid variables.\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Create an xarray DataArray for the prediction\n",
    "    data = xr.DataArray(\n",
    "        preds[i,:],\n",
    "        dims=topomask.shape,\n",
    "        name=\"prediction\",\n",
    "    )\n",
    "\n",
    "    # Add metadata or attributes if necessary\n",
    "    data.attrs[\"scenario\"] = scenario_id_str\n",
    "    data.attrs[\"description\"] = \"Model predictions for scenario.\"\n",
    "    data.attrs[]\n",
    "\n",
    "    # Save to NetCDF\n",
    "    data.to_netcdf(file_path)\n",
    "    print(f\"Saved predictions for scenario '{scenario_id_str}' to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inundation-emulator-e-rOiPvR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
